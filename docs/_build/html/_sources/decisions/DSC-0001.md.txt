# DSC-0001: Addition of _assign_sklearn_parameters function
# Date: 2024-10-27
# Decision: Use _assign_sklearn_parameters(self, model) for models
# Status: Accepted
# Motivation: Lack of getting and storing parameters from the wrappers
# of classification and regression models
# Reason: This function gets the parameters of the wrapper models and 
# stores them in a dictionary so that they can be reached 
# later when the information is needed.
# Limitations: This function will only work with models 
# which have intercept and coefficients; it will cause an error 
# if the function is called on a model without these properties.
# Alternatives: Instead of creating a function for this, get intercept and
# coefficients from each model by multiple functions (since a function is
# needed for each model).

# DSC-0002: Addition of def id(self)
# Date: 2024-10-27
# Decision: Usage of def id(self) getter function which is used 
# to generate a unique id for the object on which this function is called.
# Status: Accepted
# Motivation: Lack of function to generate a unique id for the objects.
# Reason: This function creates a unique id and can be used in classes that
# inherit from the Artifact class, so we can reach objects with their 
# unique ids.
# Limitations: The id might not guarantee uniqueness if the version 
# and asset paths are not unique.
# Alternatives: Python's built-in uuid module.

# DSC-0003: Getter functions in Artifact class
# Date: 2024-10-27
# Decision: Creating getters for all the object attributes in the Artifact
# Class
# Status: Accepted
# Motivation: Lack of safely reaching the artifact arguments such as name 
# for encapsulation.
# Reason: The getter functions allow the user to view information in a safe
# environment to prevent leakage and maintain the private nature of 
# attributes.
# Limitations: None
# Alternatives: None

# DSC-0004: Usage of TypeVar
# Date: 2024-10-27
# Decision: Adding TypeVar in the dataset class
# Status: Accepted
# Motivation: Unable to see the type hint "Dataset"
# Reason: TypeVar can allow the functions to work with a specific type,
# which in this case is "Dataset".
# Limitations: It only allows one type to work with the functions at a 
# time.
# Alternatives: Union (if we want to allow multiple types).

# DSC-0005: Usage of np.argmax
# Date: 2024-11-01
# Decision: Using np.argmax in pipeline when categorical data is one-hot 
# encoded
# Status: Accepted
# Motivation: To be able to use sklearn models (chosen for this project) 
# which require ground truth in a 1D array.
# Reason: When the one-hot encoder is used for categorical data, it places 
# each category into a separate column. This was not a 1D array as needed 
# for calculations, so np.argmax is used to group each category in one column 
# with specific labels for each category (0,0,0,1,1,2...), allowing 
# calculations to be done without any dimension errors.
# Limitations: Data needs to be one-hot encoded for np.argmax to work as
# intended.
# Alternatives: LabelEncoder

# DSC-0006: Importing is_numeric_dtype from pandas.api.types 
# Date: 2024-11-01
# Decision: Using is_numeric_type from pandas to check if a column is 
# numeric for feature detection.
# Status: Accepted
# Motivation: Lack of a function to check feature type of the columns.
# Reason: Checks if a column is numerical so that we can inform the reader
# about the feature type of selected columns, allowing proper models for
# training and predictions.
# Limitations: If a column has both numeric and categorical data, it won’t
# work properly.
# Alternatives: Use a for loop to check each row in the column with "
# is_instance()".

# DSC-0007: Usage of st.session_state 
# Date: 2024-11-01
# Decision: Using st.session_state to initialize objects, so their 
# information is stored during interactions.
# Status: Accepted
# Motivation: Without it, the object's information would be lost for the next
# interaction due to Streamlit’s behavior.
# Reason: TAs recommended using classes, making page creation complex.
# st.session_state allows the app to keep data available during runtime, 
# which is useful when we want to keep the object data reachable during
# functionality.
# Limitations: st.session_state is specific to Streamlit, so it binds the data
# to Streamlit.
# Alternatives: Use a database to persist data across interactions without
# losing information.

# DSC-0008: Addition of def _find_feature()
# Date: 2024-11-01
# Decision: Addition of a function to prevent code smells.
# Status: Accepted
# Motivation: Before this function, a code block was duplicated for the same
# functionality.
# Reason: This function finds the feature types of both input and target
# features provided by the user. It is called on input and target features,
# iterating to return their types (numerical or categorical).
# Limitations: This function is implemented in this class only; to reuse, we
# must inherit it or repeat the code.
# Alternatives: Use separate list comprehensions for each feature detection
# (may cause code repetition).

# DSC-0009: Usage of st.number_input()
# Date: 2024-11-03
# Decision: Allow users to input the split value for train-split.
# Status: Accepted
# Motivation: Allow users to change the default split value (0.8) as needed.
# Reason: This Streamlit function allows the user to input their split value
# for training based on that proportion, however we limited them to a max value 
# of 90 and min of 10 to force the test-split.
# Limitations: If no input is provided, the default split value is used.
# Alternatives: Use a dropdown menu for preset values instead of allowing
# arbitrary input.

# DSC-0010: Usage of regex for pretty printing
# Date: 2024-11-03
# Decision: Use regex to display the pipeline in a readable format.
# Status: Accepted
# Motivation: Improve readability of the pipeline summary.
# Reason: Regex extracts and formats details like model type, input features,
# target features, split ratio, and metrics selected by the user to display as
# a readable summary.
# Limitations: Regex may fail if the pipeline format changes, making it unable
# to extract key features.
# Alternatives: Use JSON to display pipeline summary in dictionary format.

# DSC-0011: Usage of pickle for saving and loading
# Date: 2024-11-03
# Decision: Use pickle to save the pipeline as an artifact.
# Status: Accepted
# Motivation: Save the pipeline in st.session_state with all data included.
# Reason: Pickle allows saving and loading across sessions, letting users
# reuse a saved model for further predictions.
# Limitations: Pickle artifacts are unreadable when opening the file path.
# Alternatives: Use JSON for pipeline storage and retrieval.

# DSC-0012: Addition of _prediction() function in deployment page
# Date: 2024-11-05
# Decision: Create a function to let users upload a CSV file and make
# predictions with the selected pipeline.
# Status: Accepted
# Motivation: Let users upload a new CSV, select features, and make predictions
# on the saved pipeline in a single function.
# Reason: This function allows the user to upload a dataset, select features,
# and make predictions in the app.
# Limitations: The pickle artifact is unreadable when opened directly.
# Alternatives: Use JSON for pipeline storage and retrieval.

# DSC-0013: creating a pipeline_options variable in Deployment page.
# Date: 2024-11-05
# Decision: Use comprehensions in order to get the name and the version
# of the saved pipeline in one option(for the user view)
# Status: Accepted
# Motivation: When we provided the dropdown menu we saw that the user 
# can only see the name of their saved pipeline, so we wanted to provide them
# with the version as well to prevent the overriding of the model
# Reason: we decided to use the comprehension since it will return a dictionary
# of keys which are the name and version of the saved pipeline, so that user 
# can choose the pipeline based on both its name and version
# Limitations: if the user saves different models with same name and version  
# this will not work since the keys must be unique.
# Alternatives: for loop to get the name and version

# DSC-0014: Removal of the .get_params() in preprocessing.py
# Date: 2024-11-08
# Decision: Returning the instance of the OneHotEncoder, StandardScaler instead
# of the parameters.
# Status: Accepted
# Motivation: We were not able to get the inverse transformation of the
# encoders without having an instance of them
# Reason: In order to display the proper and meaningful results, we decided
# the best solution would be to return the instances of the encoders. This
# allows us to use them to either get the categorical values in the correct
# order (classification) or to be able to inverse transform our values
# (regression). We noticed that we were not using the “self._artifact”, which
# allowed us to just remove the .get_params() and the pipeline already had an
# implementation (_register_artifact()) to deal with storing of the encoders.
# Limitations: If we would need to use the parameters of the encoders, we
# would have to get the stored model and then add .get_params().
# Alternatives: We could not use a scaler or we could provide the user
# with the scaled data.

# DSC-0015: addition of add_parameters() in Model class.
# Date: 2024-11-08
# Decision: Creating a setter function for the @property
# parameters() getter with a different name.
# Status: Accepted
# Motivation: If we created a setter for the getter function parameter() 
# with the same name “parameters.setter”, it would override the whole 
# dictionary without keeping the existing parameters.
# Reason: Creation of this function allows the adding and updating 
# the parameters in a safe environment while keeping the existing 
# parameters in the dictionary without overriding the information.
# Limitations: If the whole dictionary wants to be overridden 
# without keeping any existing parameters, this function won't work.
# Alternatives: Addition of update_paramters() to handle the updating of 
# the parameters dictionary.

# DSC-0016: Two getters for self._data in artifact.py
# Date: 2024-11-08
# Decision: Keep the two getters in artifact.py
# Status: Accepted
# Motivation: Since we made the self._data private, for system.py to work, it
# needs to be able to access data, This is why the getter "data" was created.
# The provided implementation of dataset requires there to be a save and a read
# method present in the parent.
# Reason: self._data has two getters, and both are needed based on provided
# child class (dataset) and system.py. Dataset required read and save methods
# to be implemented in the parent class, and system.py requires a getter 
# for data.
# Limitations: Code is repeated
# Alternatives: Change the code in system.py (we were told not to change
# anything in already implemented code).

# DSC-0017: Parameters not used for fit and predict skleran wrappers.
# Date: 2024-11-09
# Decision: Not using parameters for the predict method.
# Status: Accepted
# Motivation: The sklearn wrappers already store the necessary parameters to
# create predictions, thats why we do not store them for the purpose of 
# retrieving them and using them for the prediction. This is done automatically
# by sklearn. They are stored because of the requirements of the test_pipeline
# which requires there to be a  parameters dictionary, and in case of future
# expansions.
# Reason: Using the parameters in the predict method of sklearn wrappers
# would be redundant.
# Limitations: None, since we still assign the parameters to the parameters
# dictionary incase we would like to use it.
# Alternatives: We could use the parameters to create predictions,
# however that would be redundant.

# DSC-0018: Model ABC is tailored for sklearn
# Date: 2024-11-09
# Decision: Fit and predict in model are tailored to sklearn.
# Status: Accepted
# Motivation: In order to prevent code smells (repeating code).
# Reason: Since all of our implemented models are sklearn wrappers, we decided
# to put the generic fit and predict function into the Model ABC instead of
# creating @abstractmethods.
# Limitations: If other non-sklearn models would be implemented they would 
# have to overwrite the fit and predict methods.
# Alternatives: Put the same fit and predict methods in all the models
# (causes code smell).